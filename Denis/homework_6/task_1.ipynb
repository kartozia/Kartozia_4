{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6, Kartozia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home task: Spam detection\n",
    "\n",
    "Для заданной тестовой выборки построить модель для предсказания является ли sms сообщение спамом.  \n",
    "На заданном разбиении (df_train, df_test) ваша модель должна превзойти baseline'ы, приведенные ниже.  \n",
    "\n",
    "Чем больше baseline'ов вы превзойдете, тем выше ваша оценка\n",
    "Метрика качества F1\n",
    "\n",
    "\n",
    "baseline 1: 0.9444      bag of words + Multinomial Naive Bayes  \n",
    "baseline 2: 0.9490      symbol 3-grams with IDF and l2-norm + Logistic Regression  \n",
    "baseline 3: 0.9636      text stemming + baseline 2\n",
    "\n",
    "**baseline 4: 0.9658      text stemming + Logistic Regression (word 3-grams)  + Multinomial Naive Bayes (symbol 3-grams) **\n",
    "\n",
    "! Your results must be reproducible. Если ваша модель - стохастическая (как например LogisticRegression), то вы явно должны задавать все seed и random_state в параметрах моделей  \n",
    "! Вы должны использовать df_test только для измерения качества конечной обученной модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df = df[['v1', 'v2']]\n",
    "df = df.rename(columns={'v1': 'target', 'v2': 'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class proportions\n",
    "df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 3343, test size 2229\n"
     ]
    }
   ],
   "source": [
    "# Use  df_train for model training\n",
    "# Use df_test as  hold-out dataset for your final model perfomance estimation.\n",
    "# You cannot change  this splitting\n",
    "# All results must be reproducible\n",
    "SEED = 1337\n",
    "df_train, df_test = model_selection.train_test_split(df, test_size=0.4, random_state=SEED, shuffle=True, stratify=df.target)\n",
    "print('train size %d, test size %d' % (df_train.shape[0], df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm at work. Please call</td>\n",
       "      <td>work pleas call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>ham</td>\n",
       "      <td>I dont know why she.s not getting your messages</td>\n",
       "      <td>dont know get messag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>spam</td>\n",
       "      <td>sexy sexy cum and text me im wet and warm and ...</td>\n",
       "      <td>sexi sexi cum text im wet warm readi porn u fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi I'm sue. I am 20 years old and work as a la...</td>\n",
       "      <td>hi sue 20 year old work lapdanc love sex text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>ham</td>\n",
       "      <td>All day working day:)except saturday and sunday..</td>\n",
       "      <td>day work day except saturday sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text  \\\n",
       "2175    ham                           I'm at work. Please call   \n",
       "4798    ham    I dont know why she.s not getting your messages   \n",
       "3139   spam  sexy sexy cum and text me im wet and warm and ...   \n",
       "683    spam  Hi I'm sue. I am 20 years old and work as a la...   \n",
       "3468    ham  All day working day:)except saturday and sunday..   \n",
       "\n",
       "                                                  text2  \n",
       "2175                                    work pleas call  \n",
       "4798                               dont know get messag  \n",
       "3139  sexi sexi cum text im wet warm readi porn u fu...  \n",
       "683   hi sue 20 year old work lapdanc love sex text ...  \n",
       "3468                day work day except saturday sunday  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline4 aka hw \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# basic preprocessing\n",
    "# make lowercase, remove punctuation and make stemming\n",
    "def text_process(text):\n",
    "    text = text.lower()\n",
    "    text = regex.sub(' ', text)\n",
    "    text = [stemmer.stem(word) for word in text.split() if word not in stopwords.words('english')]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df_train['text2'] = df_train['text'].apply(text_process)\n",
    "df_test['text2'] = df_test['text'].apply(text_process)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_num(x):\n",
    "    return len(re.findall('[0-9]{4,1500}',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "{'C': 16.237767391887211, 'penalty': 'l2', 'tol': 9.9999999999999995e-07}\n",
      "0.945584092368\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done 420 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 593 out of 600 | elapsed:    9.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    9.2s finished\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100000.0, 'penalty': 'l2', 'tol': 9.9999999999999995e-07}\n",
      "0.944358465369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "df_train['count_num'] = df_train['text2'].apply(find_num)\n",
    "df_test['count_num'] = df_test['text2'].apply(find_num)\n",
    "\n",
    "\n",
    "vec_word = TfidfVectorizer(lowercase=True, analyzer='word', ngram_range=(1,3), \n",
    "                      min_df=10, # exclude 3-grams appeared in less then 10 messages. \n",
    "                      use_idf=True)\n",
    "X_train_word = vec_word.fit_transform(df_train['text2'])\n",
    "X_train_word = sparse.csr_matrix(pd.concat([pd.DataFrame(X_train_word.toarray(),columns=vec_word.get_feature_names(),index=df_train.index),df_train['count_num']],axis=1))   # Here's the initialization of the sparse matrix.\n",
    "X_test_word = vec_word.transform(df_test['text2'])\n",
    "X_test_word = sparse.csr_matrix(pd.concat([pd.DataFrame(X_test_word.toarray(),columns=vec_word.get_feature_names(),index=df_test.index),df_test['count_num']],axis=1))\n",
    "\n",
    "\n",
    "vec_char = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(3,3), min_df=5, norm='l2',\n",
    "# exclude 3-grams appeared in less then 10 messages. \n",
    "                      use_idf=True)\n",
    "X_train_char = vec_char.fit_transform(df_train['text2'])\n",
    "X_train_char = sparse.csr_matrix(pd.concat([pd.DataFrame(X_train_char.toarray(),columns=vec_char.get_feature_names(),index=df_train.index),df_train['count_num']],axis=1))   # Here's the initialization of the sparse matrix.\n",
    "X_test_char = vec_char.transform(df_test['text2'])\n",
    "X_test_char = sparse.csr_matrix(pd.concat([pd.DataFrame(X_test_char.toarray(),columns=vec_char.get_feature_names(),index=df_test.index),df_test['count_num']],axis=1))\n",
    "\n",
    "y_train = (df_train['target'] == 'spam').astype(int)\n",
    "y_test = (df_test['target'] == 'spam').astype(int)\n",
    "\n",
    "grid_word = GridSearchCV(LogisticRegression(random_state=SEED), \n",
    "                   param_grid={'C': np.logspace(-3,5,20), \n",
    "                               'penalty': ['l1','l2'],\n",
    "                               'tol':np.logspace(-6,-4,3)}, \n",
    "                    scoring='f1', n_jobs=-1, cv=5, verbose=True, return_train_score=True)\n",
    "grid_word.fit(X_train_word, y_train)\n",
    "print(grid_word.best_params_)\n",
    "print(grid_word.best_score_)\n",
    "model_word = grid_word.best_estimator_\n",
    "y_pred_word = model_word.predict_proba(X_train_word)[:,1]\n",
    "\n",
    "\n",
    "grid_char = GridSearchCV(LogisticRegression(random_state=SEED), \n",
    "                   param_grid={'C': np.logspace(-3,5,20), \n",
    "                               'penalty': ['l1','l2'],\n",
    "                               'tol':np.logspace(-6,-4,3)}, \n",
    "                    scoring='f1', n_jobs=-1, cv=5, verbose=True, return_train_score=True)\n",
    "grid_char.fit(X_train_char, y_train)\n",
    "print(grid_char.best_params_)\n",
    "print(grid_char.best_score_)\n",
    "model_char = grid_char.best_estimator_\n",
    "y_pred_char = model_char.predict_proba(X_train_char)[:,1]\n",
    "\n",
    "score_overall = []\n",
    "for i in tqdm(range(100)):\n",
    "    y_result = y_pred_word * i/100 + y_pred_char * (1 - i/100)\n",
    "    score = []\n",
    "    for threshold in range(1000):\n",
    "        y_pred = (y_result > (threshold/1000)).astype(int)\n",
    "        score.append(metrics.f1_score(y_train, y_pred))\n",
    "    score_overall.append([max(score),np.argmax(score) / 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.22500000000000001]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_overall[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96587030716723543"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = model_word.predict_proba(X_test_word)[:,1] * 0.50 + model_char.predict_proba(X_test_char)[:,1] * (1 - 0.50)\n",
    "y_pred = (y_pred_prob > 0.28).astype(int)\n",
    "metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape (3343, 6657)\n",
      "train 0.979775280899\n",
      "test 0.948096885813\n"
     ]
    }
   ],
   "source": [
    "# baseline 1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# build binary feature matrix from BoW model\n",
    "vec = TfidfVectorizer(lowercase=True, analyzer='word', ngram_range=(1,1), norm=None, use_idf=False, binary=True)\n",
    "X = vec.fit_transform(df_train.text)\n",
    "print('feature matrix shape', X.shape)\n",
    "\n",
    "# encode class labels\n",
    "label_enc = LabelEncoder().fit(df_train.target)\n",
    "y_train = label_enc.transform(df_train.target)\n",
    "\n",
    "# fit our prediction model\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(X, y_train)\n",
    "print('train', metrics.f1_score(y_train, model.predict(X)))\n",
    "\n",
    "# perfomance on test dataset\n",
    "X_test = vec.transform(df_test.text)\n",
    "y_pred = model.predict(X_test)\n",
    "y_test = label_enc.transform(df_test.target)\n",
    "print('test', metrics.f1_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape (3343, 11749)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'C': 29763.514416313192, 'penalty': 'l2'}\n",
      "best estimator 0.944847228112\n",
      "train 1.0\n",
      "test 0.949211908932\n"
     ]
    }
   ],
   "source": [
    "# baseline 2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# build feature matrix from 3-grams\n",
    "# with l2-normalization and smoothed idf - look in docs for more details\n",
    "vec = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(3,3), norm='l2', use_idf=True, smooth_idf=True)\n",
    "X = vec.fit_transform(df_train.text)\n",
    "print('feature matrix shape', X.shape)\n",
    "\n",
    "#encode labels\n",
    "label_enc = LabelEncoder().fit(df_train.target)\n",
    "y_train = label_enc.transform(df_train.target)\n",
    "\n",
    "# Logistic Regression classifier has several hyperparams\n",
    "# Optimize C (coeff before regularizer) and penalty (type of regularizer) using crossvalidation with grid search\n",
    "# Basically it means it will look over every combination of hyperparams in the specified region (or lattice)\n",
    "# and return the best one. \n",
    "# Look in docs  for more details\n",
    "grid = GridSearchCV(LogisticRegression(random_state=SEED), # our model \n",
    "                   param_grid={'C': np.logspace(0,5,20), # C in lattice [10^0...10^5]\n",
    "                               'penalty': ['l1', 'l2']}, \n",
    "                    scoring='f1', # our perfomance measure \n",
    "                    n_jobs=-1, # multithread \n",
    "                    cv=5, # 5-fold stratified cross-validation \n",
    "                    verbose=True, return_train_score=True)\n",
    "\n",
    "grid.fit(X, y_train)\n",
    "print('best params', grid.best_params_)\n",
    "print('best estimator', grid.best_score_)\n",
    "model = grid.best_estimator_\n",
    "\n",
    "# grid.best_estimator_ is already fitted on whole train dataset\n",
    "print('train', metrics.f1_score(y_train, model.predict(X)))\n",
    "\n",
    "# perfomance on test dataset\n",
    "X_test = vec.transform(df_test.text)\n",
    "y_pred = model.predict(X_test)\n",
    "y_test = label_enc.transform(df_test.target)\n",
    "print('test', metrics.f1_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline 3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# basic preprocessing\n",
    "# make lowercase, remove punctuation and make stemming\n",
    "def text_process(text):\n",
    "    text = text.lower()\n",
    "    text = regex.sub(' ', text)\n",
    "    text = [stemmer.stem(word) for word in text.split() if word not in stopwords.words('english')]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "df_train['text2'] = df_train['text'].apply(text_process)\n",
    "df_test['text2'] = df_test['text'].apply(text_process)\n",
    "df_train.head()\n",
    "\n",
    "# build feature matrix from 3-grams\n",
    "vec = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(3,3), \n",
    "                      min_df=10, # exclude 3-grams appeared in less then 10 messages. \n",
    "                      use_idf=True)\n",
    "X = vec.fit_transform(df_train.text2)\n",
    "print('feature matrix shape', X.shape)\n",
    "\n",
    "#encode labels\n",
    "label_enc = LabelEncoder().fit(df_train.target)\n",
    "y_train = label_enc.transform(df_train.target)\n",
    "\n",
    "# optimize  hyperparams\n",
    "grid = GridSearchCV(LogisticRegression(random_state=SEED), \n",
    "                   param_grid={'C': np.logspace(0,5,10), \n",
    "                               'penalty': ['l1', 'l2']}, \n",
    "                    scoring='f1', n_jobs=-1, cv=5, verbose=True, return_train_score=True)\n",
    "\n",
    "grid.fit(X, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "model = grid.best_estimator_\n",
    "print('train', metrics.f1_score(y_train, model.predict(X)))\n",
    "\n",
    "X_test = vec.transform(df_test.text2)\n",
    "y_pred = model.predict(X_test)\n",
    "y_test = label_enc.transform(df_test.target)\n",
    "print('test', metrics.f1_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
