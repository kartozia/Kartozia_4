{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from pattern.web import Wikipedia, plaintext\n",
    "from nltk.collocations import *\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def text_cleaning(self, article):\n",
    "        stop_words = ['on', 'in', 'at', 'near', 'over', 'under', 'between', 'to', 'from', 'into', 'out', 'of', 'off', 'with', 'since', 'by', 'as', 'for', 'on', 'a', 'the', 'an']\n",
    "        text = article.plaintext()\n",
    "        text = text.lower()\n",
    "        tokens = regexp_tokenize(text, r'[\\w]*', gaps=False)\n",
    "        for token in tokens:\n",
    "            if token in stop_words:\n",
    "                tokens.remove(token)\n",
    "        draft = ' '.join(tokens)\n",
    "        remove_numbers = re.sub('[\\d]{1,4}', '', draft)\n",
    "        final = re.sub('[\\s]{1,25}', ' ', remove_numbers)\n",
    "        return final\n",
    "    \n",
    "    def get_articles(self, start):\n",
    "        arr = []\n",
    "        stop_words = ['on', 'in', 'at', 'near', 'over', 'under', 'between', 'to', 'from', 'into', 'out', 'of', 'off', 'with', 'since', 'by', 'as', 'for', 'on', 'a', 'the', 'an']\n",
    "        wiki = Wikipedia(language=\"en\")\n",
    "        article = wiki.article(start)\n",
    "        arr.append(self.text_cleaning(article))\n",
    "        for title in article.links:\n",
    "            article = wiki.article(title)\n",
    "            arr.append(self.text_cleaning(article))\n",
    "        list_of_strings = ' '.join(arr)\n",
    "        return list_of_strings\n",
    "            \n",
    "            \n",
    "       # for i in range(depth):\n",
    "        #    for title in article.links:\n",
    "         #       print(set(self.get_articles(title, depth-1)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStatistics:\n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "        \n",
    "    def get_top_3grams(self, n):\n",
    "        list_of_3grams_in_descending_order_by_freq = list()\n",
    "        list_of_their_corresponding_freq = list()\n",
    "        finder = TrigramCollocationFinder.from_words(self.articles.split())\n",
    "        freqs = finder.ngram_fd\n",
    "        dictionary = {}\n",
    "        for key, value in list(freqs.items())[:n]:\n",
    "            dictionary[key] = value\n",
    "        list_of_3grams_in_descending_order_by_freq = OrderedDict(sorted(dictionary.items(), key=lambda t: t[1], reverse=True))\n",
    "        return list_of_3grams_in_descending_order_by_freq\n",
    "            \n",
    "    def get_top_words(self, n):\n",
    "        corpus = self.articles.split()\n",
    "        list_of_words_in_descending_order_by_freq = Counter(corpus)\n",
    "        return list_of_words_in_descending_order_by_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experiment(TextStatistics, WikiParser):\n",
    "    def __init__(self):\n",
    "        self.articles = 'Natural language processing'\n",
    "    def show_result(self):\n",
    "        corpus = self.get_articles(self.articles)\n",
    "        freq = TextStatistics(corpus)\n",
    "        trigram = freq.get_top_3grams(20)\n",
    "        word_top = freq.get_top_words(20)\n",
    "        #По статье \"Natural language processing\" (только по ней) считает топ-5 3-грамм и топ-5 слов.\n",
    "        stop_words = ['on', 'in', 'at', 'near', 'over', 'under', 'between', 'to', 'from', 'into', 'out', 'of', 'off', 'with', 'since', 'by', 'as', 'for', 'on', 'a', 'the', 'an']\n",
    "        wiki = Wikipedia(language=\"en\")\n",
    "        article = wiki.article(self.articles)\n",
    "        NLP_freq = TextStatistics(self.text_cleaning(article))\n",
    "        NLP_trigram = NLP_freq.get_top_3grams(5)\n",
    "        NLP_word_top = NLP_freq.get_top_words(5)\n",
    "        output = 'Топ-20 3-грамм по корпусу текстов: \\n' + str(trigram) + '\\nТоп-20 слов по корпусу текстов: \\n' + str(word_top)\n",
    "        NLP_output = 'Топ-5 3-грамм по статье NLP: \\n' + str(NLP_trigram) + '\\nТоп-5 слов по статье NLP: \\n' + str(NLP_word_top)\n",
    "        return output + '\\n' + NLP_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты метода show_result\n",
    "### Топ-20 3грамм по корпусу текстов: \n",
    "\n",
    "OrderedDict([(('natural', 'language', 'processing'), 285), (('this', 'article', 'is'), 23), (('article', 'is', 'about'), 17), (('language', 'processing', 'nlp'), 15), (('is', 'about', 'language'), 1), (('about', 'language', 'processing'), 1), (('language', 'processing', 'computers'), 1), (('processing', 'computers', 'processing'), 1), (('computers', 'processing', 'language'), 1), (('processing', 'language', 'human'), 1), (('language', 'human', 'brain'), 1), (('human', 'brain', 'see'), 1), (('brain', 'see', 'language'), 1), (('see', 'language', 'processing'), 1), (('language', 'processing', 'brain'), 1), (('processing', 'brain', 'natural'), 1), (('brain', 'natural', 'language'), 1), (('processing', 'nlp', 'is'), 1), (('nlp', 'is', 'field'), 1), (('is', 'field', 'computer'), 1)])\n",
    "\n",
    "### Топ-20 слов по корпусу текстов: \n",
    "\n",
    "[('and', 16194), ('is', 8585), ('that', 4836), ('are', 4284), ('language', 3797), ('or', 3764), ('s', 3389), ('be', 3384), ('it', 2720), ('this', 2418), ('which', 2174), ('can', 1973), ('not', 1929), ('was', 1769), ('speech', 1755), ('such', 1733), ('retrieved', 1719), ('english', 1712), ('i', 1709), ('have', 1650)]\n",
    "\n",
    "\n",
    "### Топ-5 3-грамм по статье NLP: \n",
    "\n",
    "OrderedDict([(('this', 'article', 'is'), 1), (('article', 'is', 'about'), 1), (('is', 'about', 'language'), 1), (('about', 'language', 'processing'), 1), (('language', 'processing', 'computers'), 1)])\n",
    "\n",
    "\n",
    "### Топ-5 слов по статье NLP: \n",
    "\n",
    "[('and', 70), ('language', 59), ('is', 48), ('natural', 35), ('such', 30)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
