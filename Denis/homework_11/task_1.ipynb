{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11  \n",
    "Implement Hidden Markov Model with supervised training and Viterbi algorithm for finding the most probable sequence of hidden states. Use [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) for estimation of probabilities. Apply the developed model to the problems:\n",
    "* part of speech tagging\n",
    "* spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM model for 3-grams:\n",
    "\n",
    "$P(x_1, .., x_T, y_1, .., y_T, y_{T+1}) = \\prod_{t=1}^{T+1} q(y_t | y_{t-2}, y_{t-1}) \\prod_{t=1}^T e(x_t | y_t)$\n",
    "\n",
    "$x_1, .., x_T$ - sequence of observed states of length T  \n",
    "$y_1, .., y_T$ - sequence of hidden states of length T  \n",
    "$q(i | u, v) = \\frac {count(u, v, i)} {count(u, v)} $ - transition probability   \n",
    "$e(x_k | i) = \\frac {count(i, x_k)} {count(i)}$ - emission probability  \n",
    "$A_{i,j} = A_{(u,v), j} = q(i | u, v)$ - transition matrix  \n",
    "$B_{j,k} = e(x_k | j)$ - emission matrix  \n",
    "\n",
    "We assume, that $y_{T+1} = TERM$, and $y_0 = y_{-1} = START$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    START = '*'\n",
    "    TERM = '$'\n",
    "    REST = '$REST$' # to deal with observed states who have never appeared in train dataset.\n",
    "        \n",
    "    def cond_idx(self, u, v):\n",
    "        return u + v*self.h_dim\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X - list of lists, observed states\n",
    "        y - list of lists, hidden states\n",
    "        estimate elements of A, B matrices from train sequence. \n",
    "        \"\"\"\n",
    "        \n",
    "        #######################\n",
    "        # YOUR CODE HERE\n",
    "        y_arr = [START + i + TERM for i in y]\n",
    "        self.hidden_idx2state = np.array(set(y_arr))  # lisf of unique hidden states in train sequence + [START, TERM]\n",
    "        self.hidden_states = range(len(self.hidden_idx2state))#? from state name to state index in hidden_idx2state\n",
    "        self.h_dim = len(self.hidden_states) #number of hidden states\n",
    "        \n",
    "        self.observed_idx2state = np.array(list(set(X)) + REST) # lisf of unique observed states in train sequence + [REST]\n",
    "        self.observed_states = range(len(self.observed_idx2state))#? from state name to state index in hidden_idx2state\n",
    "        self.o_dim = len(self.observed_states) #number of observed states\n",
    "        \n",
    "        #######################       \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        # estimate emission matrix\n",
    "        # YOUR CODE HERE\n",
    "        self.B = sparse.csr_matrix((self.h_dim, self.o_dim)) #sparse csr matrix of shape (h_dim, o_dim)\n",
    "        #######################\n",
    "        \n",
    "        self.B_rowsum = np.ravel(self.B.sum(axis=1))\n",
    "        \n",
    "        \n",
    "        ########################\n",
    "        # transition matrix\n",
    "        # YOUR CODE HERE\n",
    "        self.A = sparse.csr_matrix((self.h_dim**2, self.o_dim)) #dense matrix of shape (h_dim **2, h_dim)\n",
    "        # ???remember about padding for sequence of hidden states, eg {a, b} -> {START, START, a, b, TERM}\n",
    "        ########################\n",
    "        \n",
    "        self.A_rowsum = np.ravel(self.A.sum(axis=1))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def tr_prob(self, i, j):\n",
    "        \"\"\"\n",
    "        A_ij = q(j | i) = q(j| u, v) with Laplace smoothing\n",
    "        \"\"\"\n",
    "        A_ij = j/(i[0]+i[1])\n",
    "        ########################\n",
    "        # A_qs = (# transitions from state q to state s)/(# transitions from state q)\n",
    "        # result = smoothed probability\n",
    "        ########################\n",
    "        return result\n",
    "    \n",
    "    def em_prob(self, j, k):\n",
    "        \"\"\"\n",
    "        B_jk = e(x_k| j) with Laplace smoothing\n",
    "        \"\"\"\n",
    "        B_ij = k/j\n",
    "        ########################\n",
    "        #B_ij = (# of transitions from state j to observation i)/(# of states j)\n",
    "        #result = smoothed probability\n",
    "        ########################\n",
    "        return result\n",
    "        \n",
    "    def o_state(self, x):\n",
    "        \"\"\"\n",
    "        return index of obseved state\n",
    "        \"\"\"\n",
    "        return self.observed_states.get(x, self.observed_states[self.REST])\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most probable sequence of hidden states for every sequence of observed states\n",
    "        X - list of lists\n",
    "        \"\"\"\n",
    "        y_pred = [self._viterbi(seq) for seq in tqdm(X)]\n",
    "        return y_pred \n",
    "            \n",
    "    def _viterbi(self, X):\n",
    "        \"\"\"\n",
    "        X - list of observables\n",
    "        product of probabilities usually is not numerically stable\n",
    "        remember, that log(ab) = log(a) + log(b) and argmax[log(f(x))] = argmax[f(x)]\n",
    "        \n",
    "        \"\"\"   \n",
    "        T = len(X)\n",
    "        \n",
    "        # pi[t, u, v] - max probability for any state sequence ending with x_t = v and x_{t-1} = u.\n",
    "        pi = np.zeros((T + 1, self.h_dim, self.h_dim))\n",
    "        # backpointers, bp[t, u, v] = argmax probability for any state sequence ending with x_t = v and x_{t-1} = u.\n",
    "        bp = np.zeros((T + 1, self.h_dim, self.h_dim), dtype=np.int)\n",
    "        \n",
    "        ###################\n",
    "        # fill tables pi and bp\n",
    "        #pi[t, u, v] = max_{w} [ pi[t-1, w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        #bp[t, u, v] = argmax_{w} [ pi[t-1, w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # YOUR CODE HERE \n",
    "        for k in range(1, T + 1):\n",
    "            xk = self.o_state(X[k-1]) #return index of obseved state\n",
    "            for v in range(self.h_dim): #start\n",
    "                log_b_smoothed = self.em_prob(v, xk)\n",
    "                for u in range(self.h_dim): #next\n",
    "                    r = np.zeros(self.h_dim)\n",
    "                    for w in range(self.h_dim): #current\n",
    "                        log_a_smoothed = self.tr_prob([u,v], w) \n",
    "                        r[w] = [(pi[t-1, u, v]*w[v,u]*u[xk], u) for u in self.hidden_idx2state(t-2)]\n",
    "                        bp[k, u, v] = np.argmax(r)\n",
    "                        pi[k, u, v] = np.max(r)\n",
    "        ###################\n",
    "                    \n",
    "        term_idx = self.hidden_states[self.TERM]\n",
    "        \n",
    "        ###################\n",
    "        # r(u,v) = pi[T, u, v] * q(TERM | u, v)\n",
    "        # find argmax_{u, v} r(u, v)\n",
    "        # YOUR CODE HERE\n",
    "        # u, v = \n",
    "        ###################\n",
    "\n",
    "        h_states = [v, u]\n",
    "        ###################\n",
    "        # rollback backpointers\n",
    "        # y_{t-2} = bp[t, y_{t-1}, y_t]\n",
    "        # h_states is a reversed sequence of hidden states\n",
    "        # YOUR CODE HERE\n",
    "        # h_states = \n",
    "            \n",
    "        ###################        \n",
    "            \n",
    "        return [self.hidden_idx2state[i] for i in reversed(h_states[:T])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. Part of speech tagging\n",
    "\n",
    "Build Part-of-Speech tagging model using HMM. Estimate accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:3010]\n",
    "\n",
    "X_train = [[x[0] for x in y] for y in data]\n",
    "y_train = [[x[1] for x in y] for y in data]\n",
    "\n",
    "X_test = [[x[0] for x in y] for y in test_data]\n",
    "y_test = [[x[1] for x in y] for y in test_data]\n",
    "\n",
    "print('sentence: ', \" \".join(X_train[0]))\n",
    "print('tags: ', \" \".join(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):       \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hh = HMM().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy must be > 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.2 Vectorized viterbi\n",
    "Our currrent implementation of Viterbi is too slow. Let's make it vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HmmVectorized(HMM):\n",
    "    \n",
    "    def _viterbi(self, X):\n",
    "        \"\"\"\n",
    "        Vectorized version of Viterbi. Let's speed up!\n",
    "        X - list of observables\n",
    "        \"\"\"   \n",
    "        T = len(X)\n",
    "        \n",
    "        # One may notice, at every step t we only need pi[t-1, u, v] = pi_prev[u,v] to compute pi[t, u, v] = pi_curr[u,v]\n",
    "        pi_prev = np.zeros((self.h_dim, self.h_dim))\n",
    "        \n",
    "        # backpointers\n",
    "        bp = np.zeros((T + 1, self.h_dim, self.h_dim), dtype=np.int)\n",
    "        \n",
    "        a_rowsum = self.A_rowsum.reshape(self.h_dim, self.h_dim)\n",
    "        \n",
    "        ###################\n",
    "        # fill pi and bp\n",
    "        # pi_curr[u, v] = max_{w} [ pi_prev[w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # bp[t, u, v] = argmax_{w} [ pi_prev[w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # don't use tr_prob() and em_prob(), apply laplace smoothing directly here\n",
    "        # YOUR CODE HERE\n",
    "#         for k in range(1, T + 1):            \n",
    "#             xk = self.o_state(X[k-1])\n",
    "#             pi_curr = np.zeros_like(pi_prev)\n",
    "            \n",
    "#             for v in range(self.h_dim):\n",
    "#                 log_b_smoothed = \n",
    "#                 a = self.A[:, v].reshape(self.h_dim, self.h_dim)\n",
    "#                 log_a_smoothed = \n",
    "#                 r =  \n",
    "#                 bp[k, :, v] = np.argmax(r, axis=1)\n",
    "#                 pi_curr[:, v] = np.max(r, axis=1)\n",
    "                    \n",
    "#             pi_prev = pi_curr\n",
    "        ###################\n",
    "        \n",
    "        term_idx = self.hidden_states[self.TERM]\n",
    "        \n",
    "        ###################\n",
    "        # r(u,v) = pi[T, u, v] * q(TERM | u, v)\n",
    "        # find argmax_{u, v} r(u, v)\n",
    "        # express r(u,v) as matrix additions\n",
    "        # YOUR CODE HERE\n",
    "        # u, v = \n",
    "        ###################\n",
    "        \n",
    "        h_states = [v, u]\n",
    "        ###################\n",
    "        # rollback backpointers\n",
    "        # y_{t-2} = bp[t, y_{t-1}, y_t]\n",
    "        # h_states is a reversed sequence of hidden states\n",
    "        # YOUR CODE HERE\n",
    "        # h_states = \n",
    "            \n",
    "        ###################\n",
    "        \n",
    "        return [self.hidden_idx2state[i] for i in reversed(h_states[:T])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:3300]\n",
    "\n",
    "X_train = [[x[0] for x in y] for y in data]\n",
    "y_train = [[x[1] for x in y] for y in data]\n",
    "\n",
    "X_test = [[x[0] for x in y] for y in test_data]\n",
    "y_test = [[x[1] for x in y] for y in test_data]\n",
    "\n",
    "print('sentence: ', \" \".join(X_train[0]))\n",
    "print('tags: ', \" \".join(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hh = HmmVectorized().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy must be > 0.84 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Spelling correction\n",
    "\n",
    "Given data of true_char corrupted\\_char build spelling correction model using HMM.    \n",
    "There are 2 datatsets (spelling10.txt, spelling20.txt) with 10% and 20% corruption probability respectively.  \n",
    "Each dataset contains 30556 words. Use first 28000 for training and the rest for testing. Estimate accuracy on the test subset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "f1 = open('./data/spelling10.txt', 'r', encoding=\"utf-8\")\n",
    "f2 = open('./data/spelling20.txt', 'r', encoding=\"utf-8\")\n",
    "spell_data_1 = f1.read()\n",
    "spell_data_2 = f2.read()\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    raw = []\n",
    "    data = spell_data_1.split('_ _')\n",
    "    data = [i.split('\\n') for i in data]\n",
    "    for word in data:\n",
    "        word = [x for x in word if x]\n",
    "        raw.append(word)\n",
    "    return(raw)\n",
    "\n",
    "def create_data(data):\n",
    "    X = []#observed\n",
    "    y = []#hidden\n",
    "    raw = preprocess(data)\n",
    "    for word in raw:\n",
    "        letter = [i.split() for i in word]\n",
    "        obs = [i[1] for i in letter]\n",
    "        hid = [i[0] for i in letter]\n",
    "        X.append(obs)\n",
    "        y.append(hid)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% corruption probability\n",
    "X_1, y_1 = create_data(spell_data_1)\n",
    "X_train = X_1[:28000]\n",
    "y_train = y_1[:28000]\n",
    "\n",
    "X_test = X_1[28000:]\n",
    "y_test = y_1[28000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hh = HMM().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get > 93% accuracy (+3%) on spelling10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 20% corruption probability\n",
    "X_2, y_2 = create_data(spell_data_2)\n",
    "X_train = X_2[:28000]\n",
    "y_train = y_2[:28000]\n",
    "\n",
    "X_test = X_2[28000:]\n",
    "y_test = y_2[28000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hh = HMM().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get > 89% accuracy (+9%) on spelling20 dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
